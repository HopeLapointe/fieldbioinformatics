"""minion_medaka_test.py is a funcitonal test for the minion pipeline in medaka mode.

For each test dataset, it will make the following checks:

    * check for existence of the final consensus sequence
    * check for existence of the final VCF file (generated by artic_vcf_filter)
    * check all expected variants are reported
    * check no unexpected variants are reported
    * if there is an expected deletion, the consensus sequence will be checked for it's absence

NOTE:

    * only a basic grep is used for checking the deletion in the consensus - test will fail if deletion sequence is present multiple times (I should improve this part of the test...)

"""
from Bio import SeqIO
from tqdm.auto import tqdm
import argparse
import errno
import glob
import os
import pytest
import requests
import sys
import tarfile
import vcf


from . import pipeline

# help pytest resolve where test data is kept
TEST_DIR = os.path.dirname(os.path.abspath(__file__))

# dataDir is where to store the test data locally
dataDir = TEST_DIR + "/../test-data/"

# testData is a lookup of sampleIDs to download urls
testData = {
    "MT007544": "https://raw.githubusercontent.com/artic-network/fieldbioinformatics/master/test-data/MT007544/MT007544.fastq",
    "CVR1": "https://artic.s3.climb.ac.uk/validation-sets/CVR1.tgz",
}

# testVariants is a nested dict of sample IDs and their expected variants
testVariants = {
    "MT007544":
        {
            # pos: (ref, alt, type, count)
            29749: ["ACGATCGAGTG", 'A', "del", 1]
        },
    "CVR1":
        {
            241: ['C', 'T', "snp", 1],
            3037: ['C', 'T', "snp", 1],
            12733: ['C', 'T', "snp", 1],
            12733: ['C', 'T', "snp", 2], # this is reported twice due to pool groups
            14408: ['C', 'T', "snp", 1],
            23403: ['A', 'G', "snp", 1],
            27752: ['C', 'T', "snp", 1],
            28881: ('G', 'AAC', "snp"),
        },
}

# dataChecker will download the test data if not present
@pytest.fixture(scope="session", autouse=True)
def dataChecker():
    print("checking for test data...")
    for sampleID, url in testData.items():
        targetPath = dataDir + sampleID
        if os.path.exists(targetPath) == False:
            print("\tno data for {}" .format(sampleID))
            print("\tmaking dir at {}" .format(targetPath))
            os.mkdir(targetPath)
            print("\tdownloading from {}" .format(url))
            try:
                download(url, dataDir, sampleID)
            except Exception as e:
                print("download failed: ", e)
                sys.exit(1)
        else:
            print("\tfound data dir for {}" .format(sampleID))

# genCommand will create the medaka minion command
def genCommand(sampleID):
    return [
        "minion",
        "--medaka",
        "--read-file",
        TEST_DIR + "/../test-data/" + sampleID + "/*.fast[aq]",
        "--scheme-directory",
        TEST_DIR + "/../test-data/primer-schemes",
        "nCoV-2019/V1",
        sampleID
    ]

# cleanUp will remove all files generated by the pipeline for a given sampleID
def cleanUp(sampleID):
    fileList = glob.glob("{}.*" .format(sampleID))
    for filePath in fileList:
        try:
            os.remove(filePath)
        except:
            print("Error while deleting file : ", filePath)

# checkConsensus will return 1 if a subsequence is present in a consensus file, or 0 if absent
def checkConsensus(consensusFile, subSeq):
    for record in SeqIO.parse(open(consensusFile, 'r'), 'fasta'):
        if subSeq in record.seq:
            return 1
    return 0

# test_MedakaMinion is the unit test runner to test the minion pipeline in medaka mode
@pytest.mark.remote_data
def test_MedakaMinion():

    for sampleID, expVariants in testVariants.items():

        # generate the command
        cmd = genCommand(sampleID)

        # setup and run the minion parser
        parser = pipeline.init_pipeline_parser()

        # parse the arguments
        try:
            args = parser.parse_args(cmd)
        except SystemExit:
            print("failed to parse valid command for `artic minion --medaka`")
            assert False

        # run the minion pipeline
        try:
            args.func(parser, args)
        except SystemExit:
            print("artic minion exited early with an error")
            assert False

        # check the ARTIC consensus was created
        consensusFile = "%s.consensus.fasta" % sampleID
        assert os.path.exists(consensusFile) == True, "no consensus produced for {}" .format(sampleID)

        # check the ARTIC VCF was created
        vcfFile = "%s.pass.vcf.gz" % sampleID
        assert os.path.exists(
            vcfFile) == True, "no VCF produced for {}" .format(sampleID)

        # open the VCF and check the reported variants match the expected
        for record in vcf.Reader(filename=vcfFile):
            if record.POS in expVariants:
                assert record.REF == expVariants[record.POS][0], "incorrect REF reported in VCF for {} at position {}" .format(sampleID, record.POS)
                assert str(record.ALT[0]) == expVariants[record.POS][1], "incorrect ALT reported in VCF for {} at position {}" .format(sampleID, record.POS)
                
                # if this is an expected deletion, check the consensus sequence for it's absence
                if expVariants[record.POS][2] == "del":
                    assert checkConsensus(consensusFile, record.REF) == 0, "expected deletion for {} was reported but was left in consensus" .format(sampleID)

                    # also check that the VCF record is correctly labelled as DEL
                    assert record.is_deletion, "deletion for {} not formatted correctly in VCF" .format(sampleID)

                # else, check that the VCF record is correctly labelled as SNP
                if expVariants[record.POS][2] == "snp":
                    assert record.is_snp, "snp for {} not formatted correctly in VCF" .format(sampleID)

                # decrement/remove the variant from the expected list, so we can keep track of checked variants
                expVariants[record.POS][3] -= 1
                if (expVariants[record.POS][3] == 0):
                    del expVariants[record.POS]
            else:
                print("unexpected variant found for {}: {} at {}" .format(sampleID, str(record.ALT[0]), record.POS))
                assert False       

        # check we've confirmed all the expected variants         
        if len(expVariants) != 0:
            print("variants missed during test for {}" .format(sampleID))
            for key, val in expVariants:
                print("\t{}: {}" .format(key, val))
            assert False

        # clean up pipeline files
        cleanUp(sampleID)


def download(url, dataDir, sampleID):
    filename = url.rsplit('/', 1)[1] 
    with open(f'{dataDir}/{filename}', 'wb+') as f:
        response = requests.get(url, stream=True)
        total = int(response.headers.get('content-length'))
        if total is None:
            f.write(response.content)
        else:
            with tqdm(total=total, unit='B', unit_scale=True, desc=filename) as pbar:
                for data in tqdm(response.iter_content(chunk_size=1024)):
                    f.write(data)
                    pbar.update(1024)

    tar = tarfile.open(dataDir + "/" + filename, "r:gz")
    tar.extractall(dataDir)
    tar.close()
    os.remove(dataDir + "/" + filename)
